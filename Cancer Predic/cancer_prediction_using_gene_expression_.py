# -*- coding: utf-8 -*-
"""cancer prediction using gene expression .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14qETXX7HJA9Z-v7FnjiHQY4N79PUvEF3
"""

import pandas as pd
import numpy as np

"""# Data visualization"""

import matplotlib.pyplot as plt
import seaborn as sns

"""data preprocessing

"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import label_binarize
from sklearn.preprocessing import MinMaxScaler

"""**feature selection**

"""

from sklearn.feature_selection import mutual_info_classif

"""**classification**"""

from sklearn.multiclass import OneVsRestClassifier
from sklearn.ensemble import RandomForestClassifier

"""**Perform matrix**"""

from sklearn.metrics import balanced_accuracy_score,f1_score,precision_score,recall_score
from sklearn.metrics import classification_report,confusion_matrix,roc_curve,roc_auc_score

df=pd.read_csv('/content/cancer_gene_expression.csv')

print(df.shape)

print(df.columns[0:5])

print(df.head())

df.columns[-1]

import pandas as pd

df = pd.read_csv("cancer_gene_expression.csv")
print(df["Cancer_Type"].value_counts())   # shows counts of each cancer type
print("Number of unique cancer types:", df["Cancer_Type"].nunique())

datanul=df.isnull().sum()
print(datanul)
print('column with missing values:%d'%len(datanul[datanul>0]))

print(df['Cancer_Type'].value_counts())

df['Cancer_Type'].value_counts().plot(kind='bar')

df['Cancer_Type'].value_counts().plot(kind='pie')

"""Data Preprocessing"""

X=df.iloc[:,0:-1]
y=df.iloc[:,-1]

X.shape

y.shape

"""Encode labels"""

label_encoder=LabelEncoder()
label_encoder.fit(y)
y_encoded=label_encoder.transform(y)
labels=label_encoder.classes_
classes=np.unique(y_encoded)

labels

classes

"""**Data splitting**"""

X_train,X_test,y_train,y_test=train_test_split(X,y_encoded,test_size=0.2,random_state=42)

df.iloc[:,0:20].describe()

"""**Data Normalization**"""

min_max_scaler=MinMaxScaler()
X_train_norm=min_max_scaler.fit_transform(X_train)
X_test_norm=min_max_scaler.transform(X_test)

X_train.iloc[0,3]

X_train_norm[0,3]

"""Feature Selection"""

MI=mutual_info_classif(X_train_norm,y_train)

features=X_train.columns

features.shape

n_features=300
selected_score_indices=np.argsort(MI)[::-1][0:n_features]

X_train_selected=X_train_norm[:,selected_score_indices]
X_test_selected=X_test_norm[:,selected_score_indices]

X_train_selected.shape

X_test_selected.shape

"""**Classification**"""

RF=OneVsRestClassifier(RandomForestClassifier(max_features=0.2))
RF.fit(X_train_selected,y_train)
y_pred=RF.predict(X_test_selected)
pred_prob=RF.predict_proba(X_test_selected)

accuracy=np.round(balanced_accuracy_score(y_test,y_pred),4)
print('accuracy:%0.4f'%accuracy)
precision=np.round(precision_score(y_test,y_pred,average='weighted'),4)
print('precision:%0.4f'%precision)
recall=np.round(recall_score(y_test,y_pred,average='weighted'),4)
print('recall:%0.4f'%recall)
f1score=np.round(f1_score(y_test,y_pred,average='weighted'),4)
print('f1:%0.4f'%f1score)
report=classification_report(y_test,y_pred,target_names=labels)
print('/n')
print('classification_report/n/n')
print(report)

cm=confusion_matrix(y_test,y_pred)
cm_df=pd.DataFrame(cm,index=labels,columns=labels)

cm_df

sns.heatmap(cm_df,annot=True,cmap='Blues')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')

from sklearn.metrics import ConfusionMatrixDisplay

disp=ConfusionMatrixDisplay.from_estimator(RF,X_test_selected,y_test,
                                           display_labels=labels,
                                           cmap='Blues',
                                           xticks_rotation='vertical')
disp.plot()
plt.show()

from sklearn.metrics import roc_curve,auc
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
import numpy as np # Import numpy

# Assuming y_test, labels, classes, and pred_prob are available from previous steps

# Binarize y_test if not already done
if 'y_test_binarized' not in locals():
    y_test_binarized=label_binarize(y_test,classes=classes)

n_classes = y_test_binarized.shape[1] # Use the shape of the binarized y_test

fpr = dict()
tpr = dict()
roc_auc = dict()

for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], pred_prob[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], linestyle='--',
             label='%s vs Rest (AUC = %0.2f)' % (labels[i], roc_auc[i]))

plt.plot([0, 1], [0, 1], 'b--')
plt.xlim([0, 1])
plt.ylim([0, 1.05])
plt.title('Multiclass ROC curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc="lower right")
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Features and target
X = df.drop("Cancer_Type", axis=1)
y = df["Cancer_Type"]

# Encode target labels
le = LabelEncoder()
y = le.fit_transform(y)

# Scale features (important for Logistic Regression)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# Train Logistic Regression model
log_reg = LogisticRegression(max_iter=2000, multi_class="multinomial", solver="lbfgs")
log_reg.fit(X_train, y_train)

# Predictions
y_pred = log_reg.predict(X_test)

# Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=le.classes_))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Logistic Regression Confusion Matrix")
plt.show()

from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Features and target
X = df.drop("Cancer_Type", axis=1)
y = df["Cancer_Type"]

# Encode target labels
le = LabelEncoder()
y = le.fit_transform(y)

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# Train Decision Tree
dt_clf = DecisionTreeClassifier(max_depth=10, random_state=42)
dt_clf.fit(X_train, y_train)

# Predictions
y_pred = dt_clf.predict(X_test)

# Evaluation
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=le.classes_))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Decision Tree Confusion Matrix")
plt.show()

plt.figure(figsize=(20,10))
plot_tree(dt_clf, filled=True, feature_names=X.columns, class_names=le.classes_, max_depth=3)
plt.show()

import numpy as np

# Top 20 important features
importances = dt_clf.feature_importances_
indices = np.argsort(importances)[::-1][:20]
top_genes = [(X.columns[i], importances[i]) for i in indices]

print("Top 20 most important genes for classification:")
for gene, score in top_genes:
    print(f"{gene}: {score:.4f}")

# Bar plot of top 20 most important genes
import matplotlib.pyplot as plt
import numpy as np

# Already computed 'importances' and 'indices'
indices = np.argsort(importances)[::-1][:20]
genes = [X.columns[i] for i in indices]
scores = importances[indices]

plt.figure(figsize=(10,6))
plt.barh(genes, scores, color="teal")
plt.xlabel("Importance Score")
plt.title("Top 20 Important Genes (Decision Tree)")
plt.gca().invert_yaxis()  # highest at the top
plt.show()

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Reduce to 3D for 3D plotting
pca = PCA(n_components=3)
X_pca = pca.fit_transform(X)

fig = plt.figure(figsize=(8,6))
ax = fig.add_subplot(111, projection='3d') # Create a 3D subplot

for cancer in le.classes_:
    idx = (y == le.transform([cancer])[0])
    ax.scatter(X_pca[idx, 0], X_pca[idx, 1], X_pca[idx, 2], label=cancer, alpha=0.6) # Plot in 3D

ax.set_xlabel("PC1")
ax.set_ylabel("PC2")
ax.set_zlabel("PC3") # Use set_zlabel for the z-axis
plt.title("PCA of Gene Expression")
plt.legend()
plt.show()

from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X)

plt.figure(figsize=(8,6))
for cancer in le.classes_:
    idx = (y == le.transform([cancer])[0])
    plt.scatter(X_tsne[idx, 0], X_tsne[idx, 1], label=cancer, alpha=0.6)

plt.title("t-SNE of Gene Expression")
plt.legend()
plt.show()
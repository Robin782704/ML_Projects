# -*- coding: utf-8 -*-
"""lung cancer .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10u4ImflGvcn67MrTrxyLaZ5ix68BQkru
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df=pd.read_csv('/content/cancer_patient_data_sets.csv')

import pandas as pd

feature_importance = pd.Series(dt.feature_importances_,
                               index=df.drop(columns=['Level']).columns)
print(feature_importance.sort_values(ascending=False))

df.head()

print(df.isnull().sum())

# Strip column names (just in case)
df.columns = df.columns.str.strip()

# Fill missing gender with the most common value (mode)
gender_mode = df["Gender"].mode()
if not gender_mode.empty:
    df["Gender"] = df["Gender"].fillna(gender_mode[0])
else:
    print("Warning: Gender column mode is empty. Consider a different filling strategy.")
    # df["Gender"] = df["Gender"].fillna(0)

df['Age'].describe()

"""**The typical age range of lung cancer patients in the dataset, and whether the dataset has mostly younger or older patients.**

"""

df['Age'].plot(kind="kde")

df[df['Age']>72]

df['Age'].plot(kind='box')

df['Alcohol use'].plot(kind='hist')

df['Alcohol use'].skew()

"""## Since the skew is almost 0, Alcohol use among patients is nearly normally distributed. This means most patients are evenly spread across different alcohol use levels.

"""

df['Alcohol use'].plot(kind='box')

df['Level'].value_counts()

df['Level'].value_counts().plot(kind='bar')

df['Level'].value_counts().plot(kind='pie',autopct='%0.f%%')

df['Gender'].value_counts().plot(kind='pie',autopct='%0.f%%')

"""**Bivariate Analysis**"""

df.head()

pd.crosstab(df['Age'],df['chronic Lung Disease'],normalize='columns')*100

pd.crosstab(df['Age'],df['Alcohol use'],normalize='columns')*100

sns.heatmap(pd.crosstab(df['Age'],df['Alcohol use'],normalize='columns')*100)
#Each cell shows the percentage of people of a given age within a specific alcohol-use category.

pd.crosstab(df['Age'], df['Level']).plot(kind='bar')
plt.xlabel('Age')
plt.ylabel('Patient')
plt.title('Distribution of Lung Cancer Level by Age')
plt.xticks(ticks=[0, 1], rotation=0)
plt.show()

sns.scatterplot(data=df,x='Age',y='Level')

"""**MULTIVARIATE ANALYSIS**"""

sns.scatterplot(data=df,x='Age',y='Level',hue='Gender',style='Alcohol use')

sns.boxplot(x='Level',y='Age',hue='Gender',data=df)

sns.countplot(x='Gender', data=df)
plt.title('Distribution of Gender')
plt.xlabel('Gender')
plt.ylabel('Count')
plt.xticks(ticks=[0, 1], labels=['Male', 'Female'])
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import StratifiedKFold, cross_val_score
import seaborn as sns
import matplotlib.pyplot as plt

#Train Random Forest

#  Strip extra spaces from column names
df.columns = df.columns.str.strip()

#  Drop ID-like columns (they are not features)
cols_to_drop = [col for col in ["index", "Patient Id"] if col in df.columns]
df = df.drop(columns=cols_to_drop)

# Encode target
label_map = {"Low":0, "Medium":1, "High":2}
df["Level"] = df["Level"].map(label_map)

#  Encode categorical columns (like gender)
if "Gender" in df.columns:
    gender_mode = df["Gender"].mode()
    if not gender_mode.empty:
        df["Gender"] = df["Gender"].fillna(gender_mode[0])   # fill missing with mode
    else:
        # Handle the case where mode is empty (e.g., all values are NaN)
        # Fill with a default value like 0 (assuming 0 represents one gender)
        df["Gender"] = df["Gender"].fillna(0)
    df["Gender"] = df["Gender"].map({1:0, 2:1})              # encode 1 as 0 and 2 as 1 based on previous pie chart

# Handle missing values in numeric columns
df = df.fillna(df.mean(numeric_only=True))

#  Features (X) and Labels (y)
X = df.drop(columns=["Level"])
y = df["Level"]

# Fill missing values in the target variable 'y'
y = y.fillna(y.mode().iloc[0])

# Features (X) and Labels (y)
X = df.drop(columns=["Level"])
y = df["Level"]

# Fill missing values in the target variable 'y'
y = y.fillna(y.mode().iloc[0])

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Fill missing values in the target variable 'y'
y = y.fillna(y.mode().iloc[0])

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# Train Random Forest
rf = RandomForestClassifier(
    n_estimators=200,      # number of trees
    random_state=42,
    max_depth=None,        # let trees grow until fully pure
    min_samples_split=2    # minimum samples to split a node
)
rf.fit(X_train, y_train)

#  Predict on test set
y_pred = rf.predict(X_test)

#  Evaluate
acc = accuracy_score(y_test, y_pred)
print("Random Forest Accuracy: {:.2f}%".format(acc * 100))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(6,4))
sns.heatmap(
    cm, annot=True, fmt="d", cmap="Greens",
    xticklabels=["Low", "Medium", "High"],
    yticklabels=["Low", "Medium", "High"]
)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Random Forest")
plt.show()

# Stratified K-Fold Cross Validation
from sklearn.linear_model import LogisticRegression
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
# Define log_reg before using it
log_reg = LogisticRegression(max_iter=1000, random_state=42)
cv_scores = cross_val_score(log_reg, X_scaled, y, cv=cv, scoring="accuracy")

print("\nðŸ”Ž Cross-Validation Results (5 folds):")
print("Scores per fold:", np.round(cv_scores*100, 2))
print("Mean Accuracy: {:.2f}%".format(np.mean(cv_scores)*100))
print("Standard Deviation: {:.2f}%".format(np.std(cv_scores)*100))

# Logistic Regression Model
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# Train Logistic Regression
log_reg = LogisticRegression(max_iter=1000, random_state=42)
log_reg.fit(X_train, y_train)

# Predictions
y_pred_lr = log_reg.predict(X_test)

# Evaluation
acc = accuracy_score(y_test, y_pred_lr)
print("Logistic Regression Accuracy: {:.2f}%".format(acc * 100))

print("\nClassification Report:")
print(classification_report(y_test, y_pred_lr, target_names=["Low", "Medium", "High"]))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_lr)

plt.figure(figsize=(6,4))
sns.heatmap(
    cm, annot=True, fmt="d", cmap="Blues",
    xticklabels=["Low", "Medium", "High"],
    yticklabels=["Low", "Medium", "High"]
)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Logistic Regression")
plt.show()

from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
from itertools import cycle
import numpy as np

#Binarize the target for 3 classes
# Map string labels to integers if not already
class_map = {"Low":0, "Medium":1, "High":2}
y_bin = label_binarize(y, classes=[0,1,2])  # all data (for CV if needed)
n_classes = y_bin.shape[1]

y_test_bin = label_binarize(y_test, classes=[0,1,2])

# Train One-vs-Rest Logistic Regression
ovr_log_reg = OneVsRestClassifier(LogisticRegression(max_iter=1000, random_state=42))
ovr_log_reg.fit(X_train, label_binarize(y_train, classes=[0,1,2]))
y_score = ovr_log_reg.decision_function(X_test)

#  Compute ROC curve and AUC for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

#Plot all ROC curves
plt.figure(figsize=(7,6))
colors = cycle(["blue", "green", "red"])
class_names = ["Low", "Medium", "High"]

for i, color in zip(range(n_classes), colors):
    plt.plot(
        fpr[i], tpr[i], color=color, lw=2,
        label=f"{class_names[i]} (AUC = {roc_auc[i]:0.2f})"
    )
plt.plot([0, 1], [0, 1], "k--", lw=1)  # diagonal chance line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve â€“ Logistic Regression (One-vs-Rest)")
plt.legend(loc="lower right")
plt.show()

from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

#  train the model
dt = DecisionTreeClassifier(
    criterion="gini",
    max_depth=None,
    random_state=42
)
dt.fit(X_train, y_train)

#  Predictions
y_pred_dt = dt.predict(X_test)

# Evaluation
acc_dt = accuracy_score(y_test, y_pred_dt)
print(" Decision Tree Accuracy: {:.2f}%".format(acc_dt * 100))

print("\nClassification Report:")
print(classification_report(y_test, y_pred_dt,
                            target_names=["Low", "Medium", "High"]))

# Confusion Matrix Heatmap
cm = confusion_matrix(y_test, y_pred_dt)
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Oranges",
            xticklabels=["Low", "Medium", "High"],
            yticklabels=["Low", "Medium", "High"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Decision Tree")
plt.show()

#  Visualize the Tree
plt.figure(figsize=(18,8))
plot_tree(dt, filled=True,
          feature_names=df.drop(columns=["Level"]).columns,
          class_names=["Low", "Medium", "High"],
          rounded=True, fontsize=8)
plt.show()

## PCA
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.decomposition import PCA

features = ["Age", "Smoking", "Coughing of Blood"]
df_pca = df.dropna(subset=features).copy()
X = df_pca[features].astype(float).values

# Mean center
mean_vec = X.mean(axis=0)
X_centered = X - mean_vec
print("Mean vector:", mean_vec)

# Covariance matrix
cov_matrix = np.cov(X_centered, rowvar=False)
print("Covariance matrix:\n", cov_matrix)

# Eigenvalues and eigenvectors
eig_vals, eig_vecs = np.linalg.eig(cov_matrix)
idx = np.argsort(eig_vals)[::-1]
eig_vals = eig_vals[idx]
eig_vecs = eig_vecs[:, idx]
print("Eigenvalues:", eig_vals)
print("First principal component (direction):", eig_vecs[:,0])

# Project onto first principal component (1D)
pc1_coords = X_centered.dot(eig_vecs[:,0])
X_proj = np.outer(pc1_coords, eig_vecs[:,0]) + mean_vec

# Plot 3D points and their projection onto PC1
fig = plt.figure(figsize=(12,6))

ax = fig.add_subplot(1,2,1, projection='3d')
ax.scatter(X[:,0], X[:,1], X[:,2], c='C0', alpha=0.6, label='Original')
ax.scatter(X_proj[:,0], X_proj[:,1], X_proj[:,2], c='C1', alpha=0.6, label='Projected')
t = np.linspace(pc1_coords.min()*1.2, pc1_coords.max()*1.2, 200)
line = mean_vec + np.outer(t, eig_vecs[:,0])
ax.plot(line[:,0], line[:,1], line[:,2], 'k', lw=2, label='PC1 line')
ax.set_xlabel(features[0]); ax.set_ylabel(features[1]); ax.set_zlabel(features[2])
ax.set_title('3D Data and Projection on PC1')
ax.legend()

ax2 = fig.add_subplot(1,2,2)
ax2.hist(pc1_coords, bins=30, color='C2')
ax2.set_title('Distribution along PC1')
ax2.set_xlabel('Coordinate on PC1'); ax2.set_ylabel('Count')
plt.tight_layout()
plt.show()

pca_check = PCA(n_components=1)
pca_check.fit(X)
print("Sklearn first component:", pca_check.components_[0])
print("Explained variance ratio:", pca_check.explained_variance_ratio_)



## t-SNE
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

# Scale the features (important for t-SNE)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

from sklearn.manifold import TSNE

tsne = TSNE(
    n_components=2,
    perplexity=30,
    learning_rate=200,
    max_iter=1000,
    random_state=42
)
X_tsne = tsne.fit_transform(X_scaled)

# Create a DataFrame for easy plotting
tsne_df = pd.DataFrame({
    'TSNE1': X_tsne[:,0],
    'TSNE2': X_tsne[:,1],
    'Level': y    # keep original target for coloring
})

# Plot
plt.figure(figsize=(8,6))
sns.scatterplot(
    data=tsne_df,
    x='TSNE1', y='TSNE2',
    hue='Level',        # color by cancer level
    palette='Set1',
    alpha=0.8
)
plt.title("t-SNE projection of Lung Cancer Patients")
plt.xlabel("t-SNE 1")
plt.ylabel("t-SNE 2")
plt.legend(title="Cancer Level")
plt.show()

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Train an SVM model
svm_clf = SVC(
    kernel='rbf',        # radial basis kernel
    C=1.0,               # regularization
    gamma='scale',       # auto gamma
    probability=True,
    random_state=42
)
svm_clf.fit(X_train, y_train)

# Evaluation
y_pred_svm = svm_clf.predict(X_test)
acc_svm = accuracy_score(y_test, y_pred_svm)
print("SVM Accuracy: {:.2f}%".format(acc_svm * 100))

print("\nClassification Report:")
print(classification_report(y_test, y_pred_svm,
                            target_names=["Low", "Medium", "High"]))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_svm)
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Purples",
            xticklabels=["Low", "Medium", "High"],
            yticklabels=["Low", "Medium", "High"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - SVM")
plt.show()